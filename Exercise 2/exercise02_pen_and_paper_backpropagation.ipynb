{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pen & Paper: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a forward and backward pass to calculate the gradients for the weights $w_0, w_1, w_2, w_s$ in the following MLP. Each node represents one unit with a weight $w_i, i \\in \\{0, 1, 2\\}$ connecting it to the previous node. The connection from unit 0 to unit 2 is called a _skip connection_, which means unit 2 receives input from two sources and thus has an additional weight $w_s$. The weighted inputs are added before the nonlinearity is applied.\n",
    "\n",
    "**![There should be an image here. If you can't see it, you probably forgot to download the mlp.png!](mlp.png)**\n",
    "\n",
    "We assume that we want to solve a regression task. We use an L1-loss $L(\\hat{y}, y) = |y - \\hat{y}|$\n",
    "\n",
    "The nonlinearities for the first two units are rectified linear functions/units (ReLU): $g_0(z) = g_1(z) = \\begin{cases} 0, z<0\\\\ z, else \\end{cases}$.\n",
    "\n",
    "We do not use a nonlinearity for the second unit: $g_2(z_2) = z_2$.\n",
    "\n",
    "**Note:** We use the notation of the Deep Learning book here, i.e. $z = Wx+b$. If you attended the Machine Learning course, you might be used to the different notation used in the Bishop Book, where $z$ denotes the value after applying the activation function. Here, $z$ is the value before applying the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the backpropagation algorithm**\n",
    "\n",
    "Reuse equations/values when possible.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial z_2} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\dots{} \\\\\n",
    "    \\dots{} &= \\dots{} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_0} &= \\dots{} \\\\\n",
    "\\end{align*}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What difference does the skip connection make when propagating back the error?** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the gradients for the given datapoint and the given initial weights (calculating the gradients requires to calculate a forward pass first). Also calculate the weights and the loss after one gradient descent step.**\n",
    "\n",
    "$$(x_1, y_1) = (1, -3) \\\\\n",
    "w_0 = w_1 = w_2 = w_s = 0.5 \\\\\n",
    "Learning Rate = 1 \\\\  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your feedback on exercise 2.1: ** \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
