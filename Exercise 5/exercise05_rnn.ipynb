{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth Exercise (Chapter 10)\n",
    "\n",
    "This exercise focuses on recurrent neural networks (RNN). Also, we'll be using Pytorch from now on ðŸŽ‰!\n",
    "\n",
    "We will:\n",
    "- implement an LSTM cell in Pytorch,\n",
    "- train two forms of RNNs, namely:\n",
    "    - many to one: value memorization\n",
    "    - many to many: de-noising sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np  # generate training data\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Questions\n",
    "\n",
    "Before we dive into the implementation parts, let's think about a few aspects of recurrent neural networks.\n",
    "*Hint: Reading chapter 10 of the deep learning book might help with answering the questions.* Two other great resources are:\n",
    "- Andrej Karpathy's [blog post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), which, among other things, nicely explains the different variants of RNNs (one-to-one, many-to-many, etc).\n",
    "- Chris Olah's [blog post on LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "\n",
    "\n",
    "**1.** In what sense are convolutional neural networks and recurrent neural networks similar? In what sense are they different?\n",
    "\n",
    "**Answer**:  \n",
    "They are similar in the sense that they both use shared weights in the network.\n",
    "They are different because CNNs operate on a fixed-size input and produce a fixed-size output. In contrast to that, RNNs operate on arbitrary-length sequences, output arbitrary-length sequences and preserve the context in the memory over time.\n",
    "\n",
    "**2.** How can one counteract vanishing or exploding gradients in RNNs, allowing to learn long-term dependencies?\n",
    "\n",
    "**Answer**:  \n",
    "- Exploding gradients:\n",
    "    - use shrinkage methods for recurrent weight\n",
    "    - use gradient clipping when the norm exceeds a certain threshold\n",
    "- Vanishing gradients:\n",
    "    - use orthogonal initialization of weights\n",
    "    - use specially designed architectures that solve this problem (e.g. LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cell\n",
    "See chapter 10.10.1 of the DL book.\n",
    "\n",
    "In Pytorch, all layers inherit from `nn.Module` and implement the `forward` function (the `backward` pass is computed automatically). Parameters should be initialized in the constructor.\n",
    "\n",
    "To get a feeling for how layers are implemented in pytorch, you can for example take a look at the source code of the [Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) ($h = wX+b$).\n",
    "\n",
    "Your task here is to implement the LSTMCell, which takes a feature tensor and the hidden state as input and returns the new hidden state (sometimes also referred to as output) and the new cell state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from torch import randn\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"The LSTM layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # START TODO #############\n",
    "        # initialize required parameters / layers\n",
    "        # Forget\n",
    "        self.W_forget = Parameter(randn(self.hidden_size, self.hidden_size + self.input_size))\n",
    "        self.b_forget = Parameter(randn(self.hidden_size, 1))\n",
    "        # Input\n",
    "        self.W_input = Parameter(randn(self.hidden_size, self.hidden_size + self.input_size))\n",
    "        self.b_input = Parameter(randn(self.hidden_size, 1))\n",
    "        # Candidate\n",
    "        self.W_candidate = Parameter(randn(self.hidden_size, self.hidden_size + self.input_size))\n",
    "        self.b_candidate = Parameter(randn(self.hidden_size, 1))\n",
    "        # Output\n",
    "        self.W_output = Parameter(randn(self.hidden_size, self.hidden_size + self.input_size))\n",
    "        self.b_output = Parameter(randn(self.hidden_size, 1))\n",
    "        # END TODO #############\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, hx: Tuple[torch.Tensor] = None) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            x: The input tensor with shape (batch_size, feature_dim)\n",
    "            hx: The initial hidden state, optional. Is a two-tuple consisting of\n",
    "                the current hidden state and the internal cell state. Both have\n",
    "                shape (batch_size, hidden_size).\n",
    "                \n",
    "        Returns:\n",
    "            Tuple as (output_hidden, new_internal_state).\n",
    "        \"\"\"\n",
    "        if hx is None:\n",
    "            hx = self._init_hidden_state(x)\n",
    "        # START TODO #############\n",
    "        h, C = hx\n",
    "        #print(\"h:\", h.shape)\n",
    "        #print(\"x:\", x.shape)\n",
    "        h_x_concat = torch.cat([h, x], dim=1)\n",
    "        #print(\"concat:\", h_x_concat.shape)\n",
    "        # Forget\n",
    "        forget = torch.sigmoid(\n",
    "            torch.matmul(self.W_forget, h_x_concat) + self.b_forget\n",
    "        )\n",
    "        # Input\n",
    "        input = torch.sigmoid(\n",
    "            torch.matmul(self.W_input, h_x_concat) + self.b_input\n",
    "        )\n",
    "        \n",
    "        # Candidate\n",
    "        C_tilde = torch.tanh(\n",
    "            torch.matmul(self.W_candidate, h_x_concat) + self.b_candidate\n",
    "        )\n",
    "        \n",
    "        # New internal cell state (C)\n",
    "        C = forget * C + input * C_tilde\n",
    "        # New hidden state\n",
    "        o = torch.sigmoid(\n",
    "            torch.matmul(self.W_output, h_x_concat) + self.b_output\n",
    "        )\n",
    "        h = o * torch.tanh(C)\n",
    "        return h, C\n",
    "        # END TODO #############\n",
    "        \n",
    "    def _init_hidden_state(self, x):\n",
    "        \"\"\"Returns the hidden state with zeros.\n",
    "        \n",
    "        Returns:\n",
    "            A two-tuple (initial_hidden_state, initial_cell_state).\n",
    "            Both are all zeros with shape (batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        # START TODO #############\n",
    "        batch_size = x.shape[0]\n",
    "        return (\n",
    "            torch.zeros(batch_size, self.hidden_size, 1),\n",
    "            torch.zeros(batch_size, self.hidden_size, 1)\n",
    "        )\n",
    "        # END TODO #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(LSTMCell):\n",
    "    \"\"\"Convenience class that automatically iterates over the sequence.\"\"\"\n",
    "    \n",
    "    def forward(self, x: Union[np.ndarray, torch.Tensor], hx=None):\n",
    "        \"\"\"Iterate over the sequence and apply the LSTM cell.\n",
    "        \n",
    "        Args:\n",
    "            x: The input tensor with shape (seq_len, batch, input_size)\n",
    "            hx: The initial hidden state, optional. Is a two-tuple consisting of\n",
    "                the current hidden state and the internal cell state. Both have\n",
    "                shape (batch_size, hidden_size). If None, set to zero.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple as (output_stacked_hidden, (last_hidden_state, last_new_internal_state)).\n",
    "            output_stacked_hidden is the stacked output of all LSTMCells\n",
    "            (excluding the cell state!)\n",
    "        \"\"\"\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.Tensor(x, requires_grad=True)\n",
    "        # START TODO #############\n",
    "        outputs = []\n",
    "        # iterate over the sequence and apply the LSTM\n",
    "        seq_len, batch_size, input_size = x.shape\n",
    "        for idx in range(seq_len):\n",
    "            element = x[idx, :, :, None]\n",
    "            hx = LSTMCell.forward(self, element, hx)\n",
    "            last_internal_state = hx[1]\n",
    "            last_hidden_state = hx[0]\n",
    "            outputs.append(last_hidden_state)\n",
    "\n",
    "        output_stacked_hidden = torch.stack(outputs, 0)\n",
    "        return output_stacked_hidden, (last_hidden_state, last_internal_state)\n",
    "        # END TODO #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple test, let's see if the LSTM can learn to echo a value at a specific index of the sequence. If your implementation is correct, you should get around 97% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss:7.505478858947754\n",
      "test accuracy: 0.04\n",
      "epoch: 10, loss:0.07502259314060211\n",
      "test accuracy: 0.23\n",
      "epoch: 20, loss:0.03234599530696869\n",
      "test accuracy: 0.25\n",
      "epoch: 30, loss:0.033178240060806274\n",
      "test accuracy: 0.45\n",
      "epoch: 40, loss:0.021569225937128067\n",
      "test accuracy: 0.45\n",
      "epoch: 50, loss:0.008654121309518814\n",
      "test accuracy: 0.19\n",
      "epoch: 60, loss:0.006823328789323568\n",
      "test accuracy: 0.77\n",
      "epoch: 70, loss:0.005174268968403339\n",
      "test accuracy: 0.9\n",
      "epoch: 80, loss:0.0021402088459581137\n",
      "test accuracy: 0.93\n",
      "epoch: 90, loss:0.0023706480860710144\n",
      "test accuracy: 0.92\n",
      "epoch: 100, loss:0.00164138269610703\n",
      "test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Create 100 training sequences of length 10\n",
    "num_samples = 1000\n",
    "seq_length = 10\n",
    "batch_size = 5\n",
    "# we use a hidden size larger 1 as it makes training easier\n",
    "# as prediction we compute the mean over the output.\n",
    "hidden_size = 6\n",
    "training_sequences = torch.rand(seq_length, num_samples, 1)\n",
    "test_sequences = torch.rand(seq_length, 100, 1)\n",
    "model = LSTM(1, hidden_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "loss_fn = torch.nn.MSELoss(reduction='elementwise_mean')\n",
    "\n",
    "\n",
    "def accuracy(y, label, eps=1e-2):\n",
    "    assert y.shape == label.shape, (y.shape, label.shape)\n",
    "    return np.sum(np.abs(y - label) < eps) / len(y)\n",
    "\n",
    "\n",
    "for epoch in range(101):\n",
    "    for batch_idx in range(num_samples // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        batch = training_sequences[:, batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "        output = model(batch)[1][0]\n",
    "        output = output[:, :, 0]\n",
    "        labels = batch[1]  # echo the second element\n",
    "        #print(\"output:\", output.shape)\n",
    "        #print(\"labels:\", labels.shape)\n",
    "        loss = loss_fn(labels, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0: \n",
    "        print(f\"epoch: {epoch}, loss:{loss}\")\n",
    "        output = model(test_sequences)[1][0]\n",
    "        output = output[:, :, 0]\n",
    "        labels = test_sequences[1]  # echo the second element\n",
    "        acc = accuracy(np.squeeze(labels.numpy()),\n",
    "                       np.mean(output.detach().numpy(), axis=-1))\n",
    "        print(f\"test accuracy: {acc}\")\n",
    "    scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Use Case - Noise Removal\n",
    "Implement an RNN to remove noise from different sine function instances. If you didn't finish the `LSTM` implementation part, you can use `nn.LSTM` here.\n",
    "\n",
    "The goal is to remove gaussian noise from a sequence generated from a sine function.\n",
    "\n",
    "To get an idea what the data look like, plot six different sine function instances with and without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a generator for sine functions with different amplitudes, shift and frequency\n",
    "\n",
    "class RandomSineFunction:\n",
    "    \n",
    "    def __init__(self):\n",
    "        num_sines = np.random.randint(1, 4)\n",
    "        self.amplitude = np.random.uniform(0, 2, num_sines)\n",
    "        self.offsets = np.random.uniform(-np.pi, np.pi, num_sines)\n",
    "        self.frequency = np.random.uniform(0.1, 1, num_sines)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return np.array([a * np.sin(np.pi * f * x + o)\n",
    "                         for a, f, o in zip(self.amplitude, self. frequency, self.offsets)]).sum(axis=0)\n",
    "\n",
    "    \n",
    "def sample_sine_functions(num_functions):\n",
    "    return [RandomSineFunction() for _ in range(num_functions)]\n",
    "\n",
    "\n",
    "def noisy(y, noise_ratio=0.05, axes=None):\n",
    "    noise_range = np.ptp(y, axis=axes, keepdims=True) * noise_ratio\n",
    "    return y + np.random.normal(0, noise_range, size=y.shape)\n",
    "\n",
    "# START TODO #############\n",
    "# END TODO #############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define the model! Let's stack two LSTMs both with output shape (sequence_length, batch_size, hidden_size) followed by a Linear layer which takes a (sequence_length, batch_size, hidden_size) vector as input and outputs a tensor with shape (sequence_length, batch_size, 1).\n",
    "\n",
    "\n",
    "To allow the model to see some values before estimating the output, pad the sequence accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseRemovalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, shift: int = 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size: the number of units of the LSTM hidden state size.\n",
    "            shift:       the number of steps the RNN is run before its output\n",
    "                             is considered (\"many-to-many shifted to the right\").\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.shift = shift\n",
    "        # START TODO #############\n",
    "        # END TODO #############\n",
    "        \n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of noise removal.\n",
    "        \n",
    "        This function\n",
    "        1) pads the input sequence with self.interval zeros at the end,\n",
    "        2) applies an LSTM\n",
    "        3) cuts the first self.interval outputs\n",
    "        4) applies another LSTM\n",
    "        5) applies Linear layer.\n",
    "        \n",
    "        Args:\n",
    "            x: The input sequence\n",
    "        \n",
    "        Returns:\n",
    "            A torch.Tensor of shape (sequence length, batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Pad input sequence x at the end (shifted many-to-many model).\n",
    "        # This allows the model to see a few numbers before it has to guess\n",
    "        # the noiseless output.\n",
    "        \n",
    "        # START TODO #############\n",
    "        raise NotImplementedError\n",
    "        # END TODO #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_functions = 200\n",
    "sequence_length = 80\n",
    "noise_ratio = 0.05\n",
    "np.random.seed(0)\n",
    "train_functions = sample_sine_functions(num_functions)\n",
    "val_functions = sample_sine_functions(50)\n",
    "# interval on which we'll train and evaluate\n",
    "x = np.linspace(0, 5, sequence_length)\n",
    "\n",
    "\n",
    "def prepare_sequences(functions):\n",
    "    \"\"\"Convert to tensor and create noisy sequence\"\"\"\n",
    "    sequences = np.array([f(x).reshape(-1, 1) for f in functions])\n",
    "    # put the sequence into the first dimension\n",
    "    sequences = sequences.transpose([1, 0, 2])\n",
    "    # add some noise\n",
    "    noisy_sequences = noisy(sequences, noise_ratio, axes=(0, 2))\n",
    "    return torch.Tensor(sequences), torch.Tensor(noisy_sequences)\n",
    "\n",
    "\n",
    "train_sequences, noisy_train_sequences = prepare_sequences(train_functions)\n",
    "val_sequences, noisy_val_sequences = prepare_sequences(val_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='elementwise_mean')\n",
    "\n",
    "\n",
    "def plot_curves(ground_truth, noisy_sequence, model_output):\n",
    "    plt.figure(figsize=(14,3))\n",
    "    for i in range(min(len(ground_truth), 5)):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.plot(x, ground_truth[i], label=\"ground_truth\")\n",
    "        plt.plot(x, noisy_sequence[i], label=\"noisy_sequence\")\n",
    "        plt.plot(x, model_output[i], label=\"model_output\")\n",
    "    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def percentage_noise_removed(ground_truth, noisy_sequence, model_output):\n",
    "    \"\"\"Computes the percentage of noise the model removed.\"\"\"\n",
    "    return 100 * (1 - (np.abs(ground_truth - model_output).sum() /\n",
    "                  np.abs(ground_truth - noisy_sequence).sum()))\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, num_epochs, batch_size, plot=True):\n",
    "    for epoch in range(num_epochs):\n",
    "        # START TODO #############\n",
    "        # training loop here\n",
    "        # END TODO #############\n",
    "        print(f\"epoch: {epoch}, train loss:{loss}\")\n",
    "        # compute the validation loss\n",
    "        output = model(noisy_val_sequences)\n",
    "        loss = loss_fn(val_sequences, output)\n",
    "        print(f\"epoch: {epoch}, validation loss:{loss}\")\n",
    "        if epoch % 10 == 0:\n",
    "            np_tensors = [a.detach().numpy().transpose([1, 0, 2])\n",
    "                          for a in (val_sequences, noisy_val_sequences, output)]\n",
    "            if plot:\n",
    "                plot_curves(*np_tensors)\n",
    "            print(f\"{percentage_noise_removed(*np_tensors):2.4f}% of noise removed.\")\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 101\n",
    "\n",
    "\n",
    "model = NoiseRemovalModel(hidden_size=40, shift=10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# this should remove ~32% of the noise\n",
    "train(model, optimizer, scheduler, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "As a preparation for next week's lecture, play with the model's hyperparameters and try to improve the amount of noise removed. List at least three different configurations you have tried and the respective percentage of noise removed. Make sure to always create a new model and that you train and validate on the same data!\n",
    "\n",
    "| configuration        | Noise removed in percent |\n",
    "| -------------------- | --------------------------: |\n",
    "| initial configuration| 32.52%               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to create a new model each time!\n",
    "# START TODO #############\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
